# ğŸ  House Price Prediction using Linear Regression

This is a machine learning project I worked on to **predict housing prices in California** using **Linear Regression** and its regularized versions â€” **Ridge** and **Lasso** Regression.

The project helped me understand the importance of feature scaling, outlier handling, model evaluation, and the impact of regularization on performance.

---

## ğŸ“‚ Dataset

I used the **California Housing dataset**, which includes various features about housing blocks in California. It contains **no null values** and includes the following numeric columns:

- `MedInc` â€“ Median Income
- `HouseAge` â€“ Age of the houses
- `AveRooms` â€“ Average number of rooms
- `AveBedrms` â€“ Average number of bedrooms
- `Population` â€“ Population of the block
- `AveOccup` â€“ Average occupancy
- `Latitude`, `Longitude` â€“ Geographic coordinates
- `Price` â€“ Target variable (house value)

---

## ğŸ“Š Exploratory Data Analysis (EDA)

### âœ… Boxplot Insights

- **Outliers** were detected in several features like `Population`, `AveRooms`, `MedInc`, and `AveOccup`.
- `Population` was the most **skewed**, with values exceeding **35,000** in some rows â€” suggesting the need for **log transformation or normalization**.
- Other features like `Latitude` and `Longitude` showed no outliers (as expected due to geographical bounds).

### âœ… Correlation Heatmap

I used a correlation heatmap to examine **linear relationships** between features and the target:

- `MedInc` had the **highest positive correlation** with `Price` (~0.69)
- `AveRooms` and `HouseAge` also had weak positive correlations
- `Latitude` and `Longitude` had weak or negative correlations
- Multicollinearity was observed between `AveRooms` and `AveBedrms` (~0.85)

---

## âš™ï¸ Preprocessing

### âœ… Normalization

To bring all features to the **same scale**, I applied **Standardization** using `StandardScaler()`.

Why?

- Features like `Population` have values in **thousands**, while others like `HouseAge` are in **tens**.
- Normalization improved **model stability and convergence**.
- Itâ€™s essential for models like **Linear Regression**, **KNN**, **SVM**, etc.

---

## ğŸ§  Models Used

### ğŸ”¹ Linear Regression

This is my baseline model. It tries to minimize the **Mean Squared Error (MSE)** between predicted and actual values.

---

### ğŸ”¹ Ridge Regression

Used **L2 regularization** to prevent overfitting. It slightly improved performance and handled multicollinearity better.

---

### ğŸ”¹ Lasso Regression

Used **L1 regularization**, which also helps with **feature selection** by shrinking less useful coefficients to zero. Performance dropped a bit here but gave insights into feature importance.

---

## ğŸ“ˆ Model Performance

| Metric     | Linear Reg | Ridge Reg | Lasso Reg |
|------------|------------|-----------|------------|
| **MAE**    | ~0.52      | ~0.52     | ~0.68       |
| **RMSE**   | ~0.73      | ~0.72     | ~1.14       |
| **MSE**    | 0.5306     | â€“         | â€“           |
| **RÂ²**     | 0.5957     | â€“         | â€“           |

### ğŸ“Œ Insights:

- MAE and RMSE are close â†’ Not many large outliers.
- RMSE = â‚¹73k, so average error is in a **moderate range**.
- RÂ² = 59.6% â†’ My model explains ~60% of the variation in housing prices.
- Ridge outperformed Lasso slightly.
- Lasso had higher error but helped identify **less important features**.

---

## ğŸ“‰ Residual Analysis

I calculated **residuals** (`y_test - y_pred`) to evaluate model error. Plotted them to check for:

- Non-random patterns (indicating bias)
- Uneven variance (indicating heteroscedasticity)

Residuals were mostly centered around zero with no clear trend â€” a good sign.

---

## ğŸ“š What I Learned

- Importance of **scaling features** before training
- How **outliers and skewed data** affect model performance
- How **Ridge and Lasso** help deal with multicollinearity and overfitting
- How to evaluate a regression model using **MAE, MSE, RMSE, and RÂ²**

---

## ğŸš€ Next Steps

- Try **log transformation** on skewed features (like `Population`)
- Explore **Polynomial Regression** or **Tree-based models** for better performance
- Tune hyperparameters using **GridSearchCV**

---

## ğŸ› ï¸ Tools & Libraries Used

- Python ğŸ
- Pandas
- NumPy
- Scikit-learn
- Seaborn & Matplotlib for visualization

---

## ğŸ’¬ Feedback

If youâ€™ve got ideas for improving this model or want to collaborate, feel free to reach out!

---

